# Huffman-Compression
The following project is aimed to demonstrate the use of File Compression 

Huffman encoding is an algorithm devised by David A. Huffman of MIT in 1952 for compressing textual data to make a file occupy a smaller number of bytes. Though it is a relatively simple compression algorithm, Huffman is powerful enough that variations of it are still used today in computer networks, fax machines, modems, HDTV, and other areas. 

For this project, I wrote a file compression algorithm that uses binary trees and priority queues. The program allows the user to compress and decompress files using the standard Huffman algorithm for encoding and decoding.
   
Huffman encoding is an example of a lossless compression algorithm that works particularly well on text but can, in fact, be applied to any type of file. Using Huffman encoding to compress a file can reduce the storage it requires by a third, half, or even more, in some situations. 
